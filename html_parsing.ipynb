{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing HTML documents dowloaded from Proquest\n",
    "\n",
    "1. Select articles you want to turn into a csv file (I assume that you want to downlaod only full texts). \n",
    "2. Print these selected articles. \n",
    "3. Save the print version of the articles as an HTML file. \n",
    "4. Repeat this process until you've downloaded all of the target files. \n",
    "5. Put these HTML files and this code in the same directory. \n",
    "6. Run the script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_proquest(x):\n",
    "\n",
    "    # load libs \n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import re\n",
    "\n",
    "    # load files\n",
    "    \n",
    "    soup = BeautifulSoup(open(x,\"r\"), 'html.parser')\n",
    "\n",
    "    # save filtered results to new objects \n",
    "\n",
    "    doc_text = soup.findAll(\"text\")\n",
    "    doc_date = soup.findAll(\"\", {\"class\": \"abstract_Text col-xs-12 col-sm-10 col-md-10 col-lg-10\"})\n",
    "\n",
    "    # for loop over pages\n",
    "    \n",
    "    ## text \n",
    "\n",
    "    sum_text = []\n",
    "\n",
    "    for i in range(len(doc_text)):\n",
    "       sum_text.append(doc_text[i])\n",
    "\n",
    "    ## year\n",
    "\n",
    "    sum_date = []\n",
    "\n",
    "    for i in range(len(doc_date)):\n",
    "       sum_date.append(doc_date[i])\n",
    "    \n",
    "    \n",
    "    # check \n",
    "    \n",
    "    print(len(sum_text), len(sum_date))\n",
    "\n",
    "    # combine the results as a list\n",
    "\n",
    "    newspaper_list = {'text': sum_text, 'date' : sum_date}\n",
    "\n",
    "    # return\n",
    "    \n",
    "    import pandas as pd \n",
    "    \n",
    "    return(pd.DataFrame(newspaper_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get working directory \n",
    "\n",
    "import os \n",
    "\n",
    "os.chdir('/home/jae/muslim_newspapers-selected/full_version/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 1 page9.html\n",
      "100 100\n",
      "file 2 page6.html\n",
      "100 100\n",
      "file 3 page49.html\n",
      "100 100\n",
      "file 4 page15.html\n",
      "100 100\n",
      "file 5 page43.html\n",
      "100 100\n",
      "file 6 page3.html\n",
      "100 100\n",
      "file 7 page40.html\n",
      "100 100\n",
      "file 8 page22.html\n",
      "100 100\n",
      "file 9 page35.html\n",
      "100 100\n",
      "file 10 page38.html\n",
      "100 100\n",
      "file 11 page30.html\n",
      "100 100\n",
      "file 12 page19.html\n",
      "100 100\n",
      "file 13 page50.html\n",
      "100 100\n",
      "file 14 page51.html\n",
      "100 100\n",
      "file 15 page52.html\n",
      "100 100\n",
      "file 16 page21.html\n",
      "100 100\n",
      "file 17 page27.html\n",
      "100 100\n",
      "file 18 page24.html\n",
      "100 100\n",
      "file 19 page48.html\n",
      "100 100\n",
      "file 20 page20.html\n",
      "100 100\n",
      "file 21 page29.html\n",
      "100 100\n",
      "file 22 page25.html\n",
      "100 100\n",
      "file 23 page47.html\n",
      "100 100\n",
      "file 24 page37.html\n",
      "100 100\n",
      "file 25 page55.html\n",
      "100 100\n",
      "file 26 page4.html\n",
      "100 100\n",
      "file 27 page1.html\n",
      "100 100\n",
      "file 28 page57.html\n",
      "84 84\n",
      "file 29 page11.html\n",
      "100 100\n",
      "file 30 page7.html\n",
      "100 100\n",
      "file 31 page41.html\n",
      "100 100\n",
      "file 32 page34.html\n",
      "100 100\n",
      "file 33 page56.html\n",
      "100 100\n",
      "file 34 page28.html\n",
      "100 100\n",
      "file 35 page39.html\n",
      "100 100\n",
      "file 36 page23.html\n",
      "100 100\n",
      "file 37 page14.html\n",
      "100 100\n",
      "file 38 page18.html\n",
      "100 100\n",
      "file 39 page8.html\n",
      "100 100\n",
      "file 40 page32.html\n",
      "100 100\n",
      "file 41 page36.html\n",
      "100 100\n",
      "file 42 page54.html\n",
      "100 100\n",
      "file 43 page42.html\n",
      "100 100\n",
      "file 44 page53.html\n",
      "100 100\n",
      "file 45 page17.html\n",
      "100 100\n",
      "file 46 page44.html\n",
      "100 100\n",
      "file 47 page2.html\n",
      "100 100\n",
      "file 48 page5.html\n",
      "100 100\n",
      "file 49 page26.html\n",
      "100 100\n",
      "file 50 page31.html\n",
      "100 100\n",
      "file 51 page33.html\n",
      "100 100\n",
      "file 52 page16.html\n",
      "100 100\n",
      "file 53 page45.html\n",
      "100 100\n",
      "file 54 page46.html\n",
      "100 100\n",
      "file 55 page13.html\n",
      "100 100\n",
      "file 56 page12.html\n",
      "100 100\n",
      "file 57 page10.html\n",
      "100 100\n"
     ]
    }
   ],
   "source": [
    "# for loop over entire page results \n",
    "\n",
    "n = 0\n",
    "\n",
    "temp_dataset = []\n",
    "for filename in os.listdir(os.getcwd()):\n",
    "    if filename.endswith(\".html\"):\n",
    "        n  = n + 1\n",
    "        print(\"file\",n, filename)\n",
    "        temp_dataset.append(parsing_proquest(filename))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn list of dataframes into a single dataframe \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "newspaper_dataframe = pd.concat(temp_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe as a csv file\n",
    "\n",
    "newspaper_dataframe.to_csv(\"/home/jae/muslim_newspapers-selected/full_version.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
